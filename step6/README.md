
#### 任务描述


本关任务：编写一个`VGGNet`的模型结构。


#### 相关知识


为了完成本关任务，你需要掌握：1.理解卷积的感受野;2.`VGGNet`的特点和结构。

##### 卷积的感受野

简单来说，卷积后的一个值来源于初始矩阵的多大的范围，在`VGG`中有个很重要的思想就是2个`3×3`的卷积核的感受野与一个`5×5`的卷积核的感受野一样，但是参数`2×3×3`小于`5×5`，并且能有更强的表达能力，所以用小的卷积核代替大的卷积核能有更好的效果。

**重点：**这里很多人不能理解为什么2个`3×3`的卷积核的感受野与一个`5×5`的卷积核的感受野一样。举个例子，一个`5×5`的矩阵，经过一个`3×3`的卷积核，以步长为 1 卷积运算后，输出为`3×3`，再用一个`3×3`卷积核卷积运算后就是`1×1`了，与用一个`5×5`的卷积核做卷积运算的输出是一样的，从端到端的角度来看，就是把一个`5×5`的信息变成`1×1`，两个`3×3`的卷积核和一个`5×5`的卷积核都能做到这一点。

用这个非常经典的图来帮助理解一下。

![](/api/attachments/378522)

##### VGGNet 的特点和结构

###### 特点1：没有 LRN 层

在`AlexNet` 中，我们已经提过`LRN`层，局部响应归一化层了，但是在`VGG`中，作者是这么说的:`such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.`。即这个归一化没能提高模型在`ILSVRC`数据集上的表现，而且还增加了内存消耗，增长了计算时间。

###### 特点2：小卷积核

使用能捕捉到上下左右信息最小的卷积核大小`3×3`，有些层甚至使用了`1×1`的卷积核（其实相当于一个线性变换了，原文中还保证`1×1`卷积层的通道数也不变），然后每一层都会`padding`以保证输入输出的`size`（不包括通道）一样大。所有卷积层卷积的步长都是 1 。然后池化也是没有重叠的，范围是`3×3`，步长是`2×2`。

###### 特点3：数据增强

原文中在每一次训练的时候，都会从输入的图片（要经过缩放处理）中随机裁剪出`224×224`的部分，然后还会通过随机水平翻转和`RGB`值随机偏移，已达到数据增强的效果。不过呢，这毕竟还是数据增强的方法，不是`VGG`的核心特点。
###### VGGNet 结构

原文中提到了6个模型（其中`A-LRN`不用管，其提出的目的是证明`LRN`在这个网络中没啥用。），如下图：

![](/api/attachments/374727)

**其中`C`是比较经典的，我们就来实现一个**。可以看出`VGG`的特点其实就跟它论文的题目一样，就是深。因为任务是个比较小的四分类任务，数据集也不大，本次任务在全连接部分神经元数目有所改变，中间两层神经元数分别改为512和256。

不过原文中也说了，正是因为深，所以参数的初始化会影响很大，可以多尝试一下。原文中解决这个问题的方案是，先训练`A`网络，然后用`A`网络的前四层卷积层的参数和最后三层全连接层的参数作为`VGG`的初始化参数。

虽然本关只需搭建网络框架，但还是想提几句。这是我训练的最后几个`batch`的结果，9 个`batch`是一个`epoch`。损失是在当前`batch`上的损失，`acc`是在验证集上的准确率。


![](/api/attachments/379670)

可以看到效果其实不是特别好，只到了 0.8 左右的准确率。这个原因很有可能是网络对于任务而言有点过深了。我去除掉两个 512 个卷积核的部分（去掉了 6 层卷积层）后，一般效果能到 0.88 以上。



#### 编程要求

根据提示，在右侧编辑器补充代码，完成`VGGNet`的搭建。


#### 测试说明

平台会对你编写的代码进行测试：
测试你搭建的网络结构信息是否与正确的模型结构相同，如果相同则正确。


---
开始你的任务吧，祝你成功！
